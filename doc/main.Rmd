---
title: "Project 4 - Algorithm Implementation and Evaluation"
author: "Guo, Yajie  Li, Yiran  Shao, Nina  Wu, Chenyun  Xuan, Sijian"
date: "11/27/2017"
output:
  html_document: default
  pdf_document: 
    latex_engine: xelatex
---

## Step 0: Load the packages, specify directories

```{r}
setwd("~/Documents/GitHub/fall2017-project4-fall2017-proj4-grp8/doc")# if you are using mac version github desktop, if not please set wd to the place where this file located
```

## Step 1: Load, process and clean the data

```{r}
load("../output/movietrain_significance_weighting_n=9.Rdata")
load("../output/movietrain_significance_weighting_n=10.Rdata")
load("../output/movietrain_significance_weighting_n=11.Rdata")
load("../output/movietrain_significance_weighting_n=12.Rdata")
load("../output/movietrain_significance_weighting_n=14.Rdata")
load("../output/movietrain_significance_weighting_n=16.Rdata")
load("../output/webtrain_significance_weighting_n=5.Rdata")
load("../output/webtrain_significance_weighting_n=8.Rdata")
load("../output/webtrain_significance_weighting_n=9.Rdata")
load("../output/webtrain_significance_weighting_n=10.Rdata")

load("../output/web.sp.sim.Rdata")
load("../output/web.entropy.sim.Rdata")
load("../output/web.msd.sim.Rdata")
```

## Step 2: Implement memory-based algorithms

1) Similarity Weight

Spearman Correlation
```{r}

```

Entropy
```{r}

```

Mean-square-difference
```{r}

```

SimRank
```{r}

```

####Significance Weighting
#####read data
######MSweb data
```{r,eval=FALSE}
#######title: "Significance Weighting"
#######author: "Sijian Xuan"
#######date: "November 24, 2017"
#######output: html_document
webtrain_a = read.csv("../output/webtrain_a.csv", header = FALSE)
webtrain_a = unlist(webtrain_a)
sqrt(length(webtrain_a)) # = 4151
```
######Movie data
```{r,eval=FALSE}
movietrain_a = read.csv("../output/movie_train_a.csv", header = FALSE)
movietrain_a = unlist(movietrain_a)
movie_train_a = matrix(0, 5055,5055)
movie_train_a[upper.tri(movie_train_a, diag=FALSE)] = unlist(movietrain_a)
movie_train_a = t(movie_train_a)
movie_train_a[upper.tri(movie_train_a, diag=FALSE)] = unlist(movietrain_a)
# diagonal is 0
```
######Note: the values in the webtrain_a and in movietrain_a are number of co-reated items pairwise.
```{r,eval=FALSE}
print(mean(webtrain_a))
print(sd(webtrain_a))
print(mean(movie_train_a))
print(sd(movie_train_a))
```
######choose the number of co-rated items
######Web dataset
######It is reasonable that we choose 5, 8, 9 and 10 as the number of co-rated items.
#######n = 5
```{r,eval=FALSE}
webtrain_a_n_5 = webtrain_a
n = 5
for (i in 1:length(webtrain_a)){
  if (webtrain_a[i] >= n){
    webtrain_a_n_5[i] = 1
  }
  else{
    webtrain_a_n_5[i] = webtrain_a_n_5[i]/5
  }
}

webtrain_a_matrix_5 = as.numeric(webtrain_a_n_5)
webtrain_a_matrix_5 = matrix(webtrain_a_matrix_5, nrow = 4151, ncol = 4151)
diag(webtrain_a_matrix_5) = 0
save(webtrain_a_matrix_5,file = "../output/webtrain_significance_weighting_n=5.Rdata")
```
#######n=8
```{r,eval=FALSE}
webtrain_a_n_8 = webtrain_a
n = 8
for (i in 1:length(webtrain_a)){
  if (webtrain_a[i] >= n){
    webtrain_a_n_8[i] = 1
  }
  else{
    webtrain_a_n_8[i] = webtrain_a_n_5[i]/8
  }
}

webtrain_a_matrix_8 = as.numeric(webtrain_a_n_8)
webtrain_a_matrix_8 = matrix(webtrain_a_matrix_8, nrow = 4151, ncol = 4151)
diag(webtrain_a_matrix_8) = 0
save(webtrain_a_matrix_8,file = "../output/webtrain_significance_weighting_n=8.Rdata")
```
#######n=9
```{r,eval=FALSE}
webtrain_a_n_9 = webtrain_a
n = 9
for (i in 1:length(webtrain_a)){
  if (webtrain_a[i] >= n){
    webtrain_a_n_9[i] = 1
  }
  else{
    webtrain_a_n_9[i] = webtrain_a_n_9[i]/9
  }
}

webtrain_a_matrix_9 = as.numeric(webtrain_a_n_9)
webtrain_a_matrix_9 = matrix(webtrain_a_matrix_9, nrow = 4151, ncol = 4151)
diag(webtrain_a_matrix_9) = 0
save(webtrain_a_matrix_9,file = "../output/webtrain_significance_weighting_n=9.Rdata")
```
#######n=10
```{r,eval=FALSE}
webtrain_a_n_10 = webtrain_a
n = 10
for (i in 1:length(webtrain_a)){
  if (webtrain_a[i] >= n){
    webtrain_a_n_10[i] = 1
  }
  else{
    webtrain_a_n_10[i] = webtrain_a_n_10[i]/10
  }
}

webtrain_a_matrix_10 = as.numeric(webtrain_a_n_10)
webtrain_a_matrix_10 = matrix(webtrain_a_matrix_10, nrow = 4151, ncol = 4151)
diag(webtrain_a_matrix_10) = 0
save(webtrain_a_matrix_10,file = "../output/webtrain_significance_weighting_n=10.Rdata")
```
#######The propotions of values that are not 1
```{r,eval=FALSE}
i=17230801
a1 = (i - sum(webtrain_a_n_5 == 1))/i
a2 = (i - sum(webtrain_a_n_8 == 1))/i
a3 = (i - sum(webtrain_a_n_9 == 1))/i
a4 = (i - sum(webtrain_a_n_10 == 1))/i
```
######Movie dataset
#######We choose 9, 10, 11 and 12 as the number of co-rated items
#######n = 9
```{r,eval=FALSE}
movie_train_a_9 = movie_train_a
n = 9
for(x in 1:dim(movie_train_a)[1]){
  for(y in 1:dim(movie_train_a)[1]){
    if(movie_train_a_9[x,y] >= n){
      movie_train_a_9[x,y] = 1
    }
    else{
      movie_train_a_9[x,y] = movie_train_a_9[x,y]/n
    }
  }
}

save(movie_train_a_9,file = "../output/movietrain_significance_weighting_n=9.Rdata")
```
#######n = 10
```{r,eval=FALSE}
movie_train_a_10 = movie_train_a
n = 10
for(x in 1:dim(movie_train_a)[1]){
  for(y in 1:dim(movie_train_a)[1]){
    if(movie_train_a_10[x,y] >= n){
      movie_train_a_10[x,y] = 1
    }
    else{
      movie_train_a_10[x,y] = movie_train_a_10[x,y]/n
    }
  }
}

save(movie_train_a_10,file = "../output/movietrain_significance_weighting_n=10.Rdata")
```
### n = 11
```{r,eval=FALSE}
movie_train_a_11 = movie_train_a
n = 11
for(x in 1:dim(movie_train_a)[1]){
  for(y in 1:dim(movie_train_a)[1]){
    if(movie_train_a_11[x,y] >= n){
      movie_train_a_11[x,y] = 1
    }
    else{
      movie_train_a_11[x,y] = movie_train_a_11[x,y]/n
    }
  }
}

save(movie_train_a_11,file = "../output/movietrain_significance_weighting_n=11.Rdata")
```
#######n = 12
```{r,eval=FALSE}
movie_train_a_12 = movie_train_a
n = 12
for(x in 1:dim(movie_train_a)[1]){
  for(y in 1:dim(movie_train_a)[1]){
    if(movie_train_a_12[x,y] >= n){
      movie_train_a_12[x,y] = 1
    }
    else{
      movie_train_a_12[x,y] = movie_train_a_12[x,y]/n
    }
  }
}

save(movie_train_a_12,file = "../output/movietrain_significance_weighting_n=12.Rdata")
```
#######n = 14
```{r,eval=FALSE}
movie_train_a_14 = movie_train_a
n = 14
for(x in 1:dim(movie_train_a)[1]){
  for(y in 1:dim(movie_train_a)[1]){
    if(movie_train_a_14[x,y] >= n){
      movie_train_a_14[x,y] = 1
    }
    else{
      movie_train_a_14[x,y] = movie_train_a_14[x,y]/n
    }
  }
}

save(movie_train_a_14,file = "../output/movietrain_significance_weighting_n=14.Rdata")
```
### n = 16
```{r,eval=FALSE}
movie_train_a_16 = movie_train_a
n = 16
for(x in 1:dim(movie_train_a)[1]){
  for(y in 1:dim(movie_train_a)[1]){
    if(movie_train_a_16[x,y] >= n){
      movie_train_a_16[x,y] = 1
    }
    else{
      movie_train_a_16[x,y] = movie_train_a_16[x,y]/n
    }
  }
}

save(movie_train_a_16,file = "../output/movietrain_significance_weighting_n=16.Rdata")
```
#######The propotions of values that are not 1
```{r,eval=FALSE}
i=25553025
a1 = (i - sum(movie_train_a_9 == 1))/i
a2 = (i - sum(movie_train_a_10 == 1))/i
a3 = (i - sum(movie_train_a_11 == 1))/i
a4 = (i - sum(movie_train_a_12 == 1))/i
a5 = (i - sum(movie_train_a_14 == 1))/i
a6 = (i - sum(movie_train_a_16 == 1))/i
``````

Variance Weighting
```{r}

```

Combined Selecting Neighbors
```{r}
###Read data and set weight, threshold

load("../output/web.sp.sim.Rdata")
load("../output/web.etp.sim.Rdata")
load("../output/web.msdiff.sim.Rdata")
load("../output/mv.sp.sim.Rdata")
load("../output/mv.etp.sim.Rdata")
load("../output/mv.msdiff.sim.Rdata")
data1 = web.sp.sim
diag(data1) = 0
data2 = web.entropy.sim
diag(data2) = 0
data3 = web.msdiff.sim
diag(data3) = 0
data4 = mv.sp.sim
diag(data4) = 0
data5 = mv.etp.sim
diag(data5) = 0
data6 = mv.msdiff.sim
diag(data6) = 0
# dimension of 3 dataset are all 4151 dimension square matrices.
```
```{r}
hist(data1,30)
hist(data2,30)
hist(data3,30)
hist(data4,30)
hist(data5,30)
hist(data6,30)
```
```{r}
threshold1 = 0.6
threshold2 = 8
threshold3 = 0.6
threshold4 = 0.3
threshold5 = 0.6
threshold6 = 0.7
n = 20 
```
### Standard: larger than threshold and max number of estimator is n.
```{r}
aaa<- function(x,threshold){
  a1<- x[order(-x)][1:n]*(x[order(-x)][1:n]>=threshold)
  a1[a1==0]<- NA
  res<- which(x %in% a1)[1:n]
  return(res)
}
```
####web dataset, spearman
```{r}
data = data1
threshold = threshold1
result = t(apply(data, 1, function(x) aaa(x,threshold)))
value_matrix = matrix(0,dim(result)[1],dim(result)[2])
for(x in 1:4151){
  #4151 is the dimension of 'data'
  for(y in 1:n){
    if(is.na(value_matrix[x,y]) == FALSE){
      value_matrix[x,y] = data[x,result[x,y]]
    }
  }
}
save(value_matrix, file = "../output/sn_web_sp_n=20_th=0.6.Rdata")
```
####web dataset, entropy
```{r}
data = data2
threshold = threshold2
result = t(apply(data, 1, function(x) aaa(x,threshold)))
value_matrix = matrix(0,dim(result)[1],dim(result)[2])
for(x in 1:4151){
  #4151 is the dimension of 'data'
  for(y in 1:n){
    if(is.na(value_matrix[x,y]) == FALSE){
      value_matrix[x,y] = data[x,result[x,y]]
    }
  }
}
save(value_matrix, file = "../output/sn_web_entropy_n=20_th=8.Rdata")
```
###web dataset, msd
```{r}
data = data3
threshold = threshold3
result = t(apply(data, 1, function(x) aaa(x,threshold)))
value_matrix = matrix(0,dim(result)[1],dim(result)[2])
for(x in 1:4151){
  #4151 is the dimension of 'data'
  for(y in 1:n){
    if(is.na(value_matrix[x,y]) == FALSE){
      value_matrix[x,y] = data[x,result[x,y]]
    }
  }
}
save(value_matrix, file = "../output/sn_web_msd_n=20_th=0.6.Rdata")
```
###movie dataset, sp
```{r}
data = data4
threshold = threshold4
result = t(apply(data, 1, function(x) aaa(x,threshold)))
value_matrix = matrix(0,dim(result)[1],dim(result)[2])
for(x in 1:5055){
  #5055 is the dimension of 'data'
  for(y in 1:n){
    if(is.na(value_matrix[x,y]) == FALSE){
      value_matrix[x,y] = data[x,result[x,y]]
    }
  }
}
save(value_matrix, file = "../output/sn_movie_sp_n=20_th=0.3.Rdata")
```
###movie dataset, etp
```{r}
data = data5
threshold = threshold5
result = t(apply(data, 1, function(x) aaa(x,threshold)))
value_matrix = matrix(0,dim(result)[1],dim(result)[2])
for(x in 1:5055){
  #5055 is the dimension of 'data'
  for(y in 1:n){
    if(is.na(value_matrix[x,y]) == FALSE){
      value_matrix[x,y] = data[x,result[x,y]]
    }
  }
}
save(value_matrix, file = "../output/sn_movie_sp_n=20_th=0.6.Rdata")
```
###movie dataset, msd
```{r}
data = data6
threshold = threshold6
result = t(apply(data, 1, function(x) aaa(x,threshold)))
value_matrix = matrix(0,dim(result)[1],dim(result)[2])
for(x in 1:5055){
  #5055 is the dimension of 'data'
  for(y in 1:n){
    if(is.na(value_matrix[x,y]) == FALSE){
      value_matrix[x,y] = data[x,result[x,y]]
    }
  }
}
save(value_matrix, file = "../output/sn_movie_sp_n=20_th=0.7.Rdata")
```

Rating Normalization

Once the neighborhood has been selected, the ratings from those neighbors are combined to compute a prediction, after possibly scaling the ratings to a common distribution. An extension to the GroupLens algorithm is to account for the differences in spread between users' rating distributions by converting ratings to z-scores. And computing a weighted average of the z-scores as following.
```{r}

```

Obviously, we can get predictions in this step. In next step, we will compare the actual value in test data to prediction. 
## Step 3: Implement model-based algorithms

```{r}

```

## Step 4: Evaluation

Ranked scoring for the first data set.
```{r}

```

Mean absolute error(MAE) for the second data set.

Mean absolute error is the average of absolute value of the difference between the prediction and actual value from test data set. The calculation results are as follows.
```{r}

```

we can easily find that the algorithm which possesses the lowest MAE is the most efficient collaborative filtering algorithm. Here, the best one is     .

ROC sensitivity for the second data set.
```{r}

```

## Conclusion

