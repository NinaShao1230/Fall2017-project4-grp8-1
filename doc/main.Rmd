---
title: "Project 4 - Algorithm Implementation and Evaluation"
author: "Guo, Yajie  Li, Yiran  Shao, Nina  Wu, Chenyun  Xuan, Sijian"
date: "11/27/2017"
output:
  html_document: default
  pdf_document: 
    latex_engine: xelatex
---

## Step 0: Load the packages, specify directories

```{r}
setwd("~/Documents/GitHub/fall2017-project4-fall2017-proj4-grp8/doc")# if you are using mac version github desktop, if not please set wd to the place where this file located
```

## Step 1: Load, process and clean the data

```{r}
load("../output/movietrain_significance_weighting_n=9.Rdata")
load("../output/movietrain_significance_weighting_n=10.Rdata")
load("../output/movietrain_significance_weighting_n=11.Rdata")
load("../output/movietrain_significance_weighting_n=12.Rdata")
load("../output/movietrain_significance_weighting_n=14.Rdata")
load("../output/movietrain_significance_weighting_n=16.Rdata")
load("../output/webtrain_significance_weighting_n=5.Rdata")
load("../output/webtrain_significance_weighting_n=8.Rdata")
load("../output/webtrain_significance_weighting_n=9.Rdata")
load("../output/webtrain_significance_weighting_n=10.Rdata")

load("../output/web.sp.sim.Rdata")
load("../output/web.entropy.sim.Rdata")
load("../output/web.msd.sim.Rdata")
```

## Step 2: Implement memory-based algorithms

1) Similarity Weight

Spearman Correlation
```{r}

```

Entropy
```{r}

```

Mean-square-difference
```{r}

```

SimRank
```{r}

```

####Significance Weighting
#####read data
######MSweb data
```{r,eval=FALSE}
#######title: "Significance Weighting"
#######author: "Sijian Xuan"
#######date: "November 24, 2017"
#######output: html_document
webtrain_a = read.csv("../output/webtrain_a.csv", header = FALSE)
webtrain_a = unlist(webtrain_a)
sqrt(length(webtrain_a)) # = 4151
```
######Movie data
```{r,eval=FALSE}
movietrain_a = read.csv("../output/movie_train_a.csv", header = FALSE)
movietrain_a = unlist(movietrain_a)
movie_train_a = matrix(0, 5055,5055)
movie_train_a[upper.tri(movie_train_a, diag=FALSE)] = unlist(movietrain_a)
movie_train_a = t(movie_train_a)
movie_train_a[upper.tri(movie_train_a, diag=FALSE)] = unlist(movietrain_a)
# diagonal is 0
```
######Note: the values in the webtrain_a and in movietrain_a are number of co-reated items pairwise.
```{r,eval=FALSE}
print(mean(webtrain_a))
print(sd(webtrain_a))
print(mean(movie_train_a))
print(sd(movie_train_a))
```
######choose the number of co-rated items
######Web dataset
######It is reasonable that we choose 5, 8, 9 and 10 as the number of co-rated items.
#######n = 5
```{r,eval=FALSE}
webtrain_a_n_5 = webtrain_a
n = 5
for (i in 1:length(webtrain_a)){
  if (webtrain_a[i] >= n){
    webtrain_a_n_5[i] = 1
  }
  else{
    webtrain_a_n_5[i] = webtrain_a_n_5[i]/5
  }
}

webtrain_a_matrix_5 = as.numeric(webtrain_a_n_5)
webtrain_a_matrix_5 = matrix(webtrain_a_matrix_5, nrow = 4151, ncol = 4151)
diag(webtrain_a_matrix_5) = 0
save(webtrain_a_matrix_5,file = "../output/webtrain_significance_weighting_n=5.Rdata")
```
#######n=8
```{r,eval=FALSE}
webtrain_a_n_8 = webtrain_a
n = 8
for (i in 1:length(webtrain_a)){
  if (webtrain_a[i] >= n){
    webtrain_a_n_8[i] = 1
  }
  else{
    webtrain_a_n_8[i] = webtrain_a_n_5[i]/8
  }
}

webtrain_a_matrix_8 = as.numeric(webtrain_a_n_8)
webtrain_a_matrix_8 = matrix(webtrain_a_matrix_8, nrow = 4151, ncol = 4151)
diag(webtrain_a_matrix_8) = 0
save(webtrain_a_matrix_8,file = "../output/webtrain_significance_weighting_n=8.Rdata")
```
#######n=9
```{r,eval=FALSE}
webtrain_a_n_9 = webtrain_a
n = 9
for (i in 1:length(webtrain_a)){
  if (webtrain_a[i] >= n){
    webtrain_a_n_9[i] = 1
  }
  else{
    webtrain_a_n_9[i] = webtrain_a_n_9[i]/9
  }
}

webtrain_a_matrix_9 = as.numeric(webtrain_a_n_9)
webtrain_a_matrix_9 = matrix(webtrain_a_matrix_9, nrow = 4151, ncol = 4151)
diag(webtrain_a_matrix_9) = 0
save(webtrain_a_matrix_9,file = "../output/webtrain_significance_weighting_n=9.Rdata")
```
#######n=10
```{r,eval=FALSE}
webtrain_a_n_10 = webtrain_a
n = 10
for (i in 1:length(webtrain_a)){
  if (webtrain_a[i] >= n){
    webtrain_a_n_10[i] = 1
  }
  else{
    webtrain_a_n_10[i] = webtrain_a_n_10[i]/10
  }
}

webtrain_a_matrix_10 = as.numeric(webtrain_a_n_10)
webtrain_a_matrix_10 = matrix(webtrain_a_matrix_10, nrow = 4151, ncol = 4151)
diag(webtrain_a_matrix_10) = 0
save(webtrain_a_matrix_10,file = "../output/webtrain_significance_weighting_n=10.Rdata")
```
#######The propotions of values that are not 1
```{r,eval=FALSE}
i=17230801
a1 = (i - sum(webtrain_a_n_5 == 1))/i
a2 = (i - sum(webtrain_a_n_8 == 1))/i
a3 = (i - sum(webtrain_a_n_9 == 1))/i
a4 = (i - sum(webtrain_a_n_10 == 1))/i
```
######Movie dataset
#######We choose 9, 10, 11 and 12 as the number of co-rated items
#######n = 9
```{r,eval=FALSE}
movie_train_a_9 = movie_train_a
n = 9
for(x in 1:dim(movie_train_a)[1]){
  for(y in 1:dim(movie_train_a)[1]){
    if(movie_train_a_9[x,y] >= n){
      movie_train_a_9[x,y] = 1
    }
    else{
      movie_train_a_9[x,y] = movie_train_a_9[x,y]/n
    }
  }
}

save(movie_train_a_9,file = "../output/movietrain_significance_weighting_n=9.Rdata")
```
#######n = 10
```{r,eval=FALSE}
movie_train_a_10 = movie_train_a
n = 10
for(x in 1:dim(movie_train_a)[1]){
  for(y in 1:dim(movie_train_a)[1]){
    if(movie_train_a_10[x,y] >= n){
      movie_train_a_10[x,y] = 1
    }
    else{
      movie_train_a_10[x,y] = movie_train_a_10[x,y]/n
    }
  }
}

save(movie_train_a_10,file = "../output/movietrain_significance_weighting_n=10.Rdata")
```
### n = 11
```{r,eval=FALSE}
movie_train_a_11 = movie_train_a
n = 11
for(x in 1:dim(movie_train_a)[1]){
  for(y in 1:dim(movie_train_a)[1]){
    if(movie_train_a_11[x,y] >= n){
      movie_train_a_11[x,y] = 1
    }
    else{
      movie_train_a_11[x,y] = movie_train_a_11[x,y]/n
    }
  }
}

save(movie_train_a_11,file = "../output/movietrain_significance_weighting_n=11.Rdata")
```
#######n = 12
```{r,eval=FALSE}
movie_train_a_12 = movie_train_a
n = 12
for(x in 1:dim(movie_train_a)[1]){
  for(y in 1:dim(movie_train_a)[1]){
    if(movie_train_a_12[x,y] >= n){
      movie_train_a_12[x,y] = 1
    }
    else{
      movie_train_a_12[x,y] = movie_train_a_12[x,y]/n
    }
  }
}

save(movie_train_a_12,file = "../output/movietrain_significance_weighting_n=12.Rdata")
```
#######n = 14
```{r,eval=FALSE}
movie_train_a_14 = movie_train_a
n = 14
for(x in 1:dim(movie_train_a)[1]){
  for(y in 1:dim(movie_train_a)[1]){
    if(movie_train_a_14[x,y] >= n){
      movie_train_a_14[x,y] = 1
    }
    else{
      movie_train_a_14[x,y] = movie_train_a_14[x,y]/n
    }
  }
}

save(movie_train_a_14,file = "../output/movietrain_significance_weighting_n=14.Rdata")
```
### n = 16
```{r,eval=FALSE}
movie_train_a_16 = movie_train_a
n = 16
for(x in 1:dim(movie_train_a)[1]){
  for(y in 1:dim(movie_train_a)[1]){
    if(movie_train_a_16[x,y] >= n){
      movie_train_a_16[x,y] = 1
    }
    else{
      movie_train_a_16[x,y] = movie_train_a_16[x,y]/n
    }
  }
}

save(movie_train_a_16,file = "../output/movietrain_significance_weighting_n=16.Rdata")
```
#######The propotions of values that are not 1
```{r,eval=FALSE}
i=25553025
a1 = (i - sum(movie_train_a_9 == 1))/i
a2 = (i - sum(movie_train_a_10 == 1))/i
a3 = (i - sum(movie_train_a_11 == 1))/i
a4 = (i - sum(movie_train_a_12 == 1))/i
a5 = (i - sum(movie_train_a_14 == 1))/i
a6 = (i - sum(movie_train_a_16 == 1))/i
``````

Variance Weighting
```{r}

```

Selecting Neighbors
```{r}

```

Rating Normalization

Once the neighborhood has been selected, the ratings from those neighbors are combined to compute a prediction, after possibly scaling the ratings to a common distribution. An extension to the GroupLens algorithm is to account for the differences in spread between users' rating distributions by converting ratings to z-scores. And computing a weighted average of the z-scores as following.
```{r}

```

Obviously, we can get predictions in this step. In next step, we will compare the actual value in test data to prediction. 
## Step 3: Implement model-based algorithms

```{r}

```

## Step 4: Evaluation

Ranked scoring for the first data set.
```{r}

```

Mean absolute error(MAE) for the second data set.

Mean absolute error is the average of absolute value of the difference between the prediction and actual value from test data set. The calculation results are as follows.
```{r}

```

we can easily find that the algorithm which possesses the lowest MAE is the most efficient collaborative filtering algorithm. Here, the best one is     .

ROC sensitivity for the second data set.
```{r}

```

## Conclusion

